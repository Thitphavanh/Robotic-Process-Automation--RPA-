"""
News Scraper Service - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á (10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏°‡∏ß‡∏î)
"""
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from django.utils import timezone
from .models import NewsSource, NewsArticle

try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False
    print("Warning: yfinance not installed. Stock scraping will use fallback method.")


class NewsScraperService:
    """Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß - 10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏°‡∏ß‡∏î"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

    def scrape_all_categories(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏∏‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà - 10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏°‡∏ß‡∏î"""
        results = {}

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢ (10 ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó)
        results['stock_thai'] = self.scrape_thai_stocks()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® (‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤ ‡∏¢‡∏∏‡πÇ‡∏£‡∏õ ‡∏à‡∏µ‡∏ô - 10 ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®)
        results['stock_us'] = self.scrape_us_stocks()
        results['stock_europe'] = self.scrape_europe_stocks()
        results['stock_china'] = self.scrape_china_stocks()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Bitcoin/Crypto (10 ‡∏™‡∏Å‡∏∏‡∏•)
        results['crypto'] = self.scrape_crypto()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≠‡∏á
        results['gold'] = self.scrape_gold()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß Tech (10 ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡∏´‡∏°‡∏ß‡∏î)
        results['tech_ai'] = self.scrape_tech_ai()
        results['tech_hardware'] = self.scrape_tech_hardware()
        results['tech_software'] = self.scrape_tech_software()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß Football (10 ‡∏Ç‡πà‡∏≤‡∏ß)
        results['football'] = self.scrape_football_news()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß EV Car (10 ‡∏Ç‡πà‡∏≤‡∏ß)
        results['ev_car'] = self.scrape_ev_car_news()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß Rocket & Space Technology (10 ‡∏Ç‡πà‡∏≤‡∏ß)
        results['rocket_space'] = self.scrape_rocket_space_news()

        return results

    def scrape_thai_stocks(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢ - 10 ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        # TOP 10 ‡∏´‡∏∏‡πâ‡∏ô‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à
        thai_stocks = [
            ('PTT.BK', '‡∏õ‡∏ï‡∏ó.'),
            ('CPALL.BK', '‡∏ã‡∏µ‡∏û‡∏µ‡∏≠‡∏≠‡∏•‡∏•‡πå'),
            ('AOT.BK', '‡∏ó‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏¢‡∏≤‡∏ô'),
            ('KBANK.BK', '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢'),
            ('SCB.BK', '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå'),
            ('BBL.BK', '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û'),
            ('ADVANC.BK', '‡πÅ‡∏≠‡∏î‡∏ß‡∏≤‡∏ô‡∏ã‡πå ‡∏≠‡∏¥‡∏ô‡πÇ‡∏ü‡∏£‡πå ‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏ß‡∏¥‡∏™'),
            ('TRUE.BK', '‡∏ó‡∏£‡∏π ‡∏Ñ‡∏≠‡∏£‡πå‡∏õ‡∏≠‡πÄ‡∏£‡∏ä‡∏±‡πà‡∏ô'),
            ('GULF.BK', '‡∏Å‡∏±‡∏•‡∏ü‡πå ‡πÄ‡∏≠‡πá‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå‡∏à‡∏µ'),
            ('PTTEP.BK', '‡∏õ‡∏ï‡∏ó.‡∏™‡∏ú.')
        ]

        if not YFINANCE_AVAILABLE:
            print("yfinance not available, skipping Thai stocks")
            return articles

        for symbol, name in thai_stocks[:10]:
            try:
                # ‡πÉ‡∏ä‡πâ yfinance ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                ticker = yf.Ticker(symbol)
                info = ticker.info
                history = ticker.history(period='1d')

                if not history.empty:
                    price = history['Close'].iloc[-1]

                    # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô
                    prev_close = info.get('previousClose', price)

                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
                    change = price - prev_close
                    change_percent = (change / prev_close * 100) if prev_close > 0 else 0

                    url = f"https://finance.yahoo.com/quote/{symbol}"
                    image_url = f"https://logo.clearbit.com/{name.lower().replace(' ', '')}.com"

                    article_data = {
                        'title': f'{name} ({symbol}) ‡∏£‡∏≤‡∏Ñ‡∏≤ {price:.2f} ‡∏ö‡∏≤‡∏ó',
                        'content': f'‡∏´‡∏∏‡πâ‡∏ô {name} ({symbol}) ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {price:.2f} ‡∏ö‡∏≤‡∏ó ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á {change:+.2f} ({change_percent:+.2f}%)',
                        'url': url,
                        'price': price,
                        'change': change,
                        'change_percent': change_percent,
                        'image_url': image_url,
                        'published_at': timezone.now(),
                        'category': 'stock_thai'
                    }

                    articles.append(article_data)
                    print(f"‚úì Scraped {name}: ${price:.2f}")

            except Exception as e:
                print(f"Error scraping {name}: {e}")

        return articles

    def scrape_us_stocks(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤ - 10 ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        # TOP 10 ‡∏´‡∏∏‡πâ‡∏ô‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤
        us_stocks = [
            ('AAPL', 'Apple'),
            ('MSFT', 'Microsoft'),
            ('GOOGL', 'Alphabet (Google)'),
            ('AMZN', 'Amazon'),
            ('NVDA', 'NVIDIA'),
            ('TSLA', 'Tesla'),
            ('META', 'Meta (Facebook)'),
            ('BRK-B', 'Berkshire Hathaway'),
            ('JPM', 'JPMorgan Chase'),
            ('V', 'Visa')
        ]

        if not YFINANCE_AVAILABLE:
            print("yfinance not available, skipping US stocks")
            return articles

        for symbol, name in us_stocks[:10]:
            try:
                # ‡πÉ‡∏ä‡πâ yfinance ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                ticker = yf.Ticker(symbol)
                info = ticker.info
                history = ticker.history(period='1d')

                if not history.empty:
                    price = history['Close'].iloc[-1]

                    # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô
                    prev_close = info.get('previousClose', price)

                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
                    change = price - prev_close
                    change_percent = (change / prev_close * 100) if prev_close > 0 else 0

                    url = f"https://finance.yahoo.com/quote/{symbol}"
                    image_url = f"https://logo.clearbit.com/{name.split()[0].lower()}.com"

                    article_data = {
                        'title': f'{name} ({symbol}) ${price:,.2f}',
                        'content': f'‡∏´‡∏∏‡πâ‡∏ô {name} ({symbol}) ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ${price:,.2f} ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á {change:+.2f} ({change_percent:+.2f}%)',
                        'url': url,
                        'price': price,
                        'change': change,
                        'change_percent': change_percent,
                        'image_url': image_url,
                        'published_at': timezone.now(),
                        'category': 'stock_us'
                    }

                    articles.append(article_data)
                    print(f"‚úì Scraped {name}: ${price:.2f}")

            except Exception as e:
                print(f"Error scraping {name}: {e}")

        return articles

    def scrape_europe_stocks(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏¢‡∏∏‡πÇ‡∏£‡∏õ - 10 ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        # TOP 10 ‡∏´‡∏∏‡πâ‡∏ô‡∏¢‡∏∏‡πÇ‡∏£‡∏õ
        europe_stocks = [
            ('MC.PA', 'LVMH (‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™)'),
            ('ASML.AS', 'ASML (‡πÄ‡∏ô‡πÄ‡∏ò‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏ô‡∏î‡πå)'),
            ('NVO', 'Novo Nordisk (‡πÄ‡∏î‡∏ô‡∏°‡∏≤‡∏£‡πå‡∏Å)'),
            ('SAP', 'SAP (‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏ô‡∏µ)'),
            ('OR.PA', "L'Or√©al (‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™)"),
            ('SAN.PA', 'Sanofi (‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™)'),
            ('SIE.DE', 'Siemens (‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏ô‡∏µ)'),
            ('NESN.SW', 'Nestl√© (‡∏™‡∏ß‡∏¥‡∏ï‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏ô‡∏î‡πå)'),
            ('NOVN.SW', 'Novartis (‡∏™‡∏ß‡∏¥‡∏ï‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏ô‡∏î‡πå)'),
            ('SHEL.L', 'Shell (‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)')
        ]

        if not YFINANCE_AVAILABLE:
            print("yfinance not available, skipping Europe stocks")
            return articles

        for symbol, name in europe_stocks[:10]:
            try:
                # ‡πÉ‡∏ä‡πâ yfinance ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                ticker = yf.Ticker(symbol)
                info = ticker.info
                history = ticker.history(period='1d')

                if not history.empty:
                    price = history['Close'].iloc[-1]

                    # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô
                    prev_close = info.get('previousClose', price)

                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
                    change = price - prev_close
                    change_percent = (change / prev_close * 100) if prev_close > 0 else 0

                    url = f"https://finance.yahoo.com/quote/{symbol}"
                    image_url = f"https://logo.clearbit.com/{name.split()[0].lower()}.com"

                    article_data = {
                        'title': f'{name} ({symbol}) ‚Ç¨{price:,.2f}',
                        'content': f'‡∏´‡∏∏‡πâ‡∏ô {name} ({symbol}) ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‚Ç¨{price:,.2f} ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á {change:+.2f} ({change_percent:+.2f}%)',
                        'url': url,
                        'price': price,
                        'change': change,
                        'change_percent': change_percent,
                        'image_url': image_url,
                        'published_at': timezone.now(),
                        'category': 'stock_europe'
                    }

                    articles.append(article_data)
                    print(f"‚úì Scraped {name}: ‚Ç¨{price:.2f}")

            except Exception as e:
                print(f"Error scraping {name}: {e}")

        return articles

    def scrape_china_stocks(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏à‡∏µ‡∏ô - 10 ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        # TOP 10 ‡∏´‡∏∏‡πâ‡∏ô‡∏à‡∏µ‡∏ô
        china_stocks = [
            ('BABA', 'Alibaba'),
            ('TCEHY', 'Tencent'),
            ('JD', 'JD.com'),
            ('BIDU', 'Baidu'),
            ('NIO', 'NIO'),
            ('XPEV', 'XPeng'),
            ('LI', 'Li Auto'),
            ('PDD', 'Pinduoduo'),
            ('NTES', 'NetEase'),
            ('TME', 'Tencent Music')
        ]

        if not YFINANCE_AVAILABLE:
            print("yfinance not available, skipping China stocks")
            return articles

        for symbol, name in china_stocks[:10]:
            try:
                # ‡πÉ‡∏ä‡πâ yfinance ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                ticker = yf.Ticker(symbol)
                info = ticker.info
                history = ticker.history(period='1d')

                if not history.empty:
                    price = history['Close'].iloc[-1]

                    # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô
                    prev_close = info.get('previousClose', price)

                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
                    change = price - prev_close
                    change_percent = (change / prev_close * 100) if prev_close > 0 else 0

                    url = f"https://finance.yahoo.com/quote/{symbol}"
                    image_url = f"https://logo.clearbit.com/{name.split()[0].lower()}.com"

                    article_data = {
                        'title': f'{name} ({symbol}) ${price:,.2f}',
                        'content': f'‡∏´‡∏∏‡πâ‡∏ô {name} ({symbol}) ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ${price:,.2f} ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á {change:+.2f} ({change_percent:+.2f}%)',
                        'url': url,
                        'price': price,
                        'change': change,
                        'change_percent': change_percent,
                        'image_url': image_url,
                        'published_at': timezone.now(),
                        'category': 'stock_china'
                    }

                    articles.append(article_data)
                    print(f"‚úì Scraped {name}: ${price:.2f}")

            except Exception as e:
                print(f"Error scraping {name}: {e}")

        return articles

    def scrape_crypto(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Cryptocurrency - 10 ‡∏™‡∏Å‡∏∏‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        try:
            # ‡πÉ‡∏ä‡πâ CoinGecko API (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á API Key)
            url = "https://api.coingecko.com/api/v3/coins/markets"
            params = {
                'vs_currency': 'usd',
                'order': 'market_cap_desc',
                'per_page': 10,
                'page': 1,
                'sparkline': False,
                'price_change_percentage': '24h'
            }

            response = requests.get(url, params=params, timeout=10)

            if response.status_code == 200:
                data = response.json()

                for coin in data[:10]:
                    coin_name = coin.get('name', '')
                    symbol = coin.get('symbol', '').upper()
                    usd_price = coin.get('current_price', 0)
                    change_24h = coin.get('price_change_24h', 0)  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                    change_percent = coin.get('price_change_percentage_24h', 0)
                    market_cap = coin.get('market_cap', 0)
                    image_url = coin.get('image', '')  # ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å CoinGecko

                    article_data = {
                        'title': f'{coin_name} ({symbol}) ${usd_price:,.2f}',
                        'content': f'{coin_name} ({symbol}) ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ${usd_price:,.2f} ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á 24h: {change_24h:+.2f} ({change_percent:+.2f}%) Market Cap: ${market_cap:,.0f}',
                        'url': f'https://www.coingecko.com/en/coins/{coin.get("id", "")}',
                        'price': usd_price,
                        'change': change_24h,
                        'change_percent': change_percent,
                        'image_url': image_url,
                        'published_at': timezone.now(),
                        'category': 'crypto'
                    }

                    articles.append(article_data)

        except Exception as e:
            print(f"Error scraping crypto: {e}")

        return articles

    def scrape_gold(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥"""
        articles = []

        if not YFINANCE_AVAILABLE:
            print("yfinance not available, skipping gold")
            return articles

        try:
            # ‡πÉ‡∏ä‡πâ yfinance ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥ (GC=F = Gold Futures)
            ticker = yf.Ticker("GC=F")
            info = ticker.info
            history = ticker.history(period='1d')

            if not history.empty:
                price = history['Close'].iloc[-1]

                # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô
                prev_close = info.get('previousClose', price)

                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
                change = price - prev_close
                change_percent = (change / prev_close * 100) if prev_close > 0 else 0

                url = "https://finance.yahoo.com/quote/GC=F"

                article_data = {
                    'title': f'‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥ ${price:.2f}/oz',
                    'content': f'‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà ${price:.2f} ‡∏ï‡πà‡∏≠‡∏≠‡∏≠‡∏ô‡∏ã‡πå ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á {change:+.2f} ({change_percent:+.2f}%)',
                    'url': url,
                    'price': price,
                    'change': change,
                    'change_percent': change_percent,
                    'published_at': timezone.now(),
                    'category': 'gold'
                }

                articles.append(article_data)
                print(f"‚úì Scraped Gold: ${price:.2f}/oz")

        except Exception as e:
            print(f"Error scraping gold: {e}")

        return articles

    def scrape_tech_ai(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß AI - 10 ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        sources = [
            'https://techcrunch.com/category/artificial-intelligence/',
            'https://www.theverge.com/ai-artificial-intelligence'
        ]

        for source_url in sources:
            try:
                response = requests.get(source_url, headers=self.headers, timeout=10)

                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    article_links = soup.select('article h2 a, .duet--article--title-segment a')[:5]

                    for link in article_links:
                        try:
                            title = link.text.strip()
                            article_url = link.get('href', '')

                            if not article_url.startswith('http'):
                                article_url = source_url.split('/category')[0] + article_url

                            article_data = {
                                'title': title,
                                'content': f'‡∏Ç‡πà‡∏≤‡∏ß AI: {title}',
                                'url': article_url,
                                'published_at': timezone.now(),
                                'category': 'tech_ai'
                            }

                            articles.append(article_data)

                        except Exception as e:
                            print(f"Error parsing AI article: {e}")

            except Exception as e:
                print(f"Error scraping AI news: {e}")

        return articles[:10]

    def scrape_tech_hardware(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß Hardware - 10 ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        try:
            url = 'https://www.tomshardware.com/'
            response = requests.get(url, headers=self.headers, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                article_links = soup.select('article h3 a, .article-name a')[:10]

                for link in article_links:
                    try:
                        title = link.text.strip()
                        article_url = link.get('href', '')

                        if not article_url.startswith('http'):
                            article_url = 'https://www.tomshardware.com' + article_url

                        article_data = {
                            'title': title,
                            'content': f'‡∏Ç‡πà‡∏≤‡∏ß Hardware: {title}',
                            'url': article_url,
                            'published_at': timezone.now(),
                            'category': 'tech_hardware'
                        }

                        articles.append(article_data)

                    except Exception as e:
                        print(f"Error parsing hardware article: {e}")

        except Exception as e:
            print(f"Error scraping hardware news: {e}")

        return articles[:10]

    def scrape_tech_software(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß Software - 10 ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        try:
            url = 'https://www.zdnet.com/topic/software/'
            response = requests.get(url, headers=self.headers, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                article_links = soup.select('article h3 a, .story__title a')[:10]

                for link in article_links:
                    try:
                        title = link.text.strip()
                        article_url = link.get('href', '')

                        if not article_url.startswith('http'):
                            article_url = 'https://www.zdnet.com' + article_url

                        article_data = {
                            'title': title,
                            'content': f'‡∏Ç‡πà‡∏≤‡∏ß Software: {title}',
                            'url': article_url,
                            'published_at': timezone.now(),
                            'category': 'tech_software'
                        }

                        articles.append(article_data)

                    except Exception as e:
                        print(f"Error parsing software article: {e}")

        except Exception as e:
            print(f"Error scraping software news: {e}")

        return articles[:10]

    def scrape_football_news(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß Football - 10 ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        try:
            url = "https://www.espn.com/soccer/"
            response = requests.get(url, headers=self.headers, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                article_links = soup.select('.contentItem__title a')[:10]

                for link in article_links:
                    try:
                        title = link.text.strip()
                        article_url = link.get('href', '')

                        if not article_url.startswith('http'):
                            article_url = 'https://www.espn.com' + article_url

                        article_data = {
                            'title': title,
                            'content': f'‡∏Ç‡πà‡∏≤‡∏ß‡∏ü‡∏∏‡∏ï‡∏ö‡∏≠‡∏•: {title}',
                            'url': article_url,
                            'published_at': timezone.now(),
                            'category': 'football'
                        }

                        articles.append(article_data)

                    except Exception as e:
                        print(f"Error parsing football article: {e}")

        except Exception as e:
            print(f"Error scraping football news: {e}")

        return articles[:10]

    def scrape_ev_car_news(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß EV Car (‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÑ‡∏ü‡∏ü‡πâ‡∏≤) - 10 ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        sources = [
            ('https://electrek.co/', '.article-title a'),
            ('https://insideevs.com/', 'h3.title a')
        ]

        for source_url, selector in sources:
            try:
                response = requests.get(source_url, headers=self.headers, timeout=10)

                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    article_links = soup.select(selector)[:5]

                    for link in article_links:
                        try:
                            title = link.text.strip()
                            article_url = link.get('href', '')

                            if not article_url.startswith('http'):
                                base_url = source_url.rstrip('/')
                                article_url = base_url + article_url

                            article_data = {
                                'title': title,
                                'content': f'‡∏Ç‡πà‡∏≤‡∏ß EV Car: {title}',
                                'url': article_url,
                                'published_at': timezone.now(),
                                'category': 'ev_car'
                            }

                            articles.append(article_data)

                        except Exception as e:
                            print(f"Error parsing EV car article: {e}")

            except Exception as e:
                print(f"Error scraping EV car news from {source_url}: {e}")

        return articles[:10]

    def scrape_rocket_space_news(self):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß Rocket & Space Technology - 10 ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        articles = []

        sources = [
            ('https://www.space.com/news', 'article h3 a'),
            ('https://spacenews.com/', '.c-title__link')
        ]

        for source_url, selector in sources:
            try:
                response = requests.get(source_url, headers=self.headers, timeout=10)

                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    article_links = soup.select(selector)[:5]

                    for link in article_links:
                        try:
                            title = link.text.strip()
                            article_url = link.get('href', '')

                            if not article_url.startswith('http'):
                                if source_url.endswith('/news'):
                                    base_url = source_url.rsplit('/news', 1)[0]
                                else:
                                    base_url = source_url.rstrip('/')
                                article_url = base_url + article_url

                            article_data = {
                                'title': title,
                                'content': f'‡∏Ç‡πà‡∏≤‡∏ß Rocket & Space: {title}',
                                'url': article_url,
                                'published_at': timezone.now(),
                                'category': 'rocket_space'
                            }

                            articles.append(article_data)

                        except Exception as e:
                            print(f"Error parsing rocket/space article: {e}")

            except Exception as e:
                print(f"Error scraping rocket/space news from {source_url}: {e}")

        return articles[:10]

    def save_articles(self, articles_by_category):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß"""
        saved_articles = []
        updated_count = 0
        created_count = 0

        for category, articles in articles_by_category.items():
            for article_data in articles:
                try:
                    # ‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á NewsSource
                    source, created = NewsSource.objects.get_or_create(
                        name=f"Auto Source - {category}",
                        category=article_data.get('category', category),
                        defaults={
                            'url': article_data['url'],
                            'is_active': True
                        }
                    )

                    # ‡∏´‡∏≤ article ‡∏ó‡∏µ‡πà‡∏°‡∏µ URL ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                    article, created = NewsArticle.objects.get_or_create(
                        url=article_data['url'],
                        defaults={
                            'source': source,
                            'title': article_data['title'],
                            'content': article_data['content'],
                            'price': article_data.get('price'),
                            'change': article_data.get('change'),
                            'change_percent': article_data.get('change_percent'),
                            'image_url': article_data.get('image_url'),
                            'published_at': article_data['published_at'],
                            'scraped_at': timezone.now()
                        }
                    )

                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà record ‡πÉ‡∏´‡∏°‡πà ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                    if not created:
                        article.title = article_data['title']
                        article.content = article_data['content']
                        article.price = article_data.get('price')
                        article.change = article_data.get('change')
                        article.change_percent = article_data.get('change_percent')
                        article.image_url = article_data.get('image_url')
                        article.published_at = article_data['published_at']
                        article.scraped_at = timezone.now()
                        article.save()
                        updated_count += 1
                    else:
                        created_count += 1

                    saved_articles.append(article)

                except Exception as e:
                    print(f"Error saving article: {e}")

        print(f"üìä Summary: Created {created_count}, Updated {updated_count} articles")
        return saved_articles


# ========== Standalone Functions ==========


def scrape_all_news_sources():
    """
    Standalone function to scrape all news sources
    Used by views and tasks
    """
    scraper = NewsScraperService()

    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏´‡∏°‡∏ß‡∏î
    articles_by_category = scraper.scrape_all_categories()

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    saved_articles = scraper.save_articles(articles_by_category)

    return {
        'total_scraped': sum(len(articles) for articles in articles_by_category.values()),
        'total_saved': len(saved_articles),
        'categories': list(articles_by_category.keys())
    }
